{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dipanjanS/mastering-intelligent-agents-langgraph-workshop-dhs2025/blob/main/Module-1-Introduction-to-Generative-AI-and-Agentic-AI/M1LC1_LLM_Input_Output_and_Prompting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQJKL7ZzQMni"
   },
   "source": [
    "# LLM Input/Ouput and Prompting\n",
    "\n",
    "This section introduces the foundational concepts of using Language Models (LLMs) and Chat Models in LangChain, focusing on how to structure inputs and extract meaningful outputs. It covers:\n",
    "\n",
    "\n",
    "- **Model I/O Fundamentals**:\n",
    "  - **LLMs**: Accept plain text and return plain text, useful for traditional completion tasks.\n",
    "  - **Chat Models**: Take structured messages (like user/assistant roles) and return conversational outputs.\n",
    "  - **Prompts**: Structured inputs designed to guide LLM behavior effectively.\n",
    "  - **Output Parsers**: Used to extract structured data or formats from raw model outputs.\n",
    "\n",
    "A simple demo uses the `ChatOpenAI` class to invoke the GPT-4o-mini model with a sample prompt and display its response, setting the stage for deeper exploration into LangChain workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1KvMtf54l0d"
   },
   "source": [
    "## Install OpenAI and LangChain dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2evPp14fy258"
   },
   "outputs": [],
   "source": [
    "!pip install langchain==0.3.27 langchain-openai==0.3.29 langchain-community==0.3.27 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCiHAEmyxhnl"
   },
   "source": [
    "## Enter API Keys & Setup Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E066sqIn1YEJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# OpenAI API Key (for chat & embeddings)\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key (https://platform.openai.com/account/api-keys):\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqX0BkkWZ_e0"
   },
   "source": [
    "# Model I/O\n",
    "\n",
    "In LangChain, the central part of any application is the language model. This module provides crucial tools for working effectively with any language model, ensuring it integrates smoothly and communicates well.\n",
    "\n",
    "### Key Components of Model I/O\n",
    "\n",
    "**LLMs and Chat Models (used interchangeably):**\n",
    "- **LLMs:**\n",
    "  - **Definition:** Pure text completion models.\n",
    "  - **Input/Output:** Receives a text string and returns a text string.\n",
    "- **Chat Models:**\n",
    "  - **Definition:** Based on a language model but with different input and output types.\n",
    "  - **Input/Output:** Takes a list of chat messages as input and produces a chat message as output.\n",
    "- **Prompts:** Helps in creating adaptable and context-sensitive prompts that direct the responses of the language model.\n",
    "- **Output Parsers:** Helps in extracting and shaping information from the outputs of language models. This is valuable for turning the language model's raw output into structured data or specific formats needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kai2VrJ1m7q9"
   },
   "source": [
    "## Chat Models and LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v8nnrOGxZ2uZ"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QDJ8dDFlbWP4"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Explain what is Agentic AI in 2 bullets?\"\"\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tYym2ioWb-Mw"
   },
   "outputs": [],
   "source": [
    "response = chatgpt.invoke(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uehA4Tcdco8G"
   },
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lHfySxeCeiGq"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W8ghfHSebkpJ"
   },
   "outputs": [],
   "source": [
    "chatgpt = ChatOpenAI(model_name=\"gpt-5-mini\",\n",
    "                     reasoning = {\n",
    "                        \"effort\": \"medium\",  # 'low', 'medium', or 'high'\n",
    "                        \"summary\": \"detailed\",  # 'detailed', 'auto', or None\n",
    "                     },\n",
    "                     temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6-8oPBwbtAz"
   },
   "outputs": [],
   "source": [
    "response = chatgpt.invoke(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-chQCphTdHqM"
   },
   "outputs": [],
   "source": [
    "display(Markdown(response.additional_kwargs['reasoning']['summary'][0]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DMQgHyz3bxjM"
   },
   "outputs": [],
   "source": [
    "display(Markdown(response.content[0]['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7kLCJevjCe_"
   },
   "source": [
    "## Message Types\n",
    "\n",
    "ChatModels process a list of messages, receiving them as input and responding with a message. Messages are characterized by a few distinct types and properties:\n",
    "\n",
    "- **Role:** Indicates who is speaking in the message. LangChain offers different message classes for various roles.\n",
    "- **Content:** The substance of the message, which can vary:\n",
    "  - A string (commonly handled by most models)\n",
    "  - A list of dictionaries (for multi-modal inputs, where each dictionary details the type and location of the input)\n",
    "\n",
    "Additionally, messages have an `additional_kwargs` property, used for passing extra information specific to the message provider, not typically general. A well-known example is `function_call` from OpenAI.\n",
    "\n",
    "### Specific Message Types\n",
    "\n",
    "- **HumanMessage:** A user-generated message, usually containing only content.\n",
    "- **AIMessage:** A message from the model, potentially including `additional_kwargs`, like `tool_calls` for invoking OpenAI tools.\n",
    "- **SystemMessage:** A message from the system instructing model behavior, typically containing only content. Not all models support this type.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16MX2f01vk8B"
   },
   "source": [
    "### Simple Conversational Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bCLGlujf-s0J"
   },
   "outputs": [],
   "source": [
    "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKDMkowiiNjG"
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "prompt = \"\"\"Can you explain what is P/E Ratio in 2 lines\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Act as a helpful assistant who simplifies things with easy to understand examples.\"),\n",
    "    HumanMessage(content=prompt),\n",
    "]\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xizcAMNKjYSk"
   },
   "outputs": [],
   "source": [
    "response = chatgpt.invoke(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BrYfYw8Ijhjz"
   },
   "outputs": [],
   "source": [
    "display(Markdown(response.content.replace('$', '\\\\$')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6eTDPRWHfYVw"
   },
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R5qvveyhjksA"
   },
   "outputs": [],
   "source": [
    "messages.append(response)\n",
    "\n",
    "prompt = \"\"\"What did we discuss so far?\"\"\"\n",
    "new_message = ('user', prompt) # OR HumanMessage(content=prompt)\n",
    "messages.append(new_message)\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8dQjk-Cj15m"
   },
   "outputs": [],
   "source": [
    "response = chatgpt.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8i8KIVTwnD6Y"
   },
   "source": [
    "## Prompt Templates\n",
    "Prompt templates are pre-designed formats used to generate prompts for language models. These templates can include instructions, few-shot examples, and specific contexts and questions suited for particular tasks.\n",
    "\n",
    "LangChain provides tools for creating and using prompt templates. It aims to develop model-agnostic templates to facilitate the reuse of existing templates across different language models. Typically, these models expect prompts in the form of either a string or a list of chat messages.\n",
    "\n",
    "### Types of Prompt Templates\n",
    "\n",
    "- **PromptTemplate:**\n",
    "  - Used for creating string-based prompts.\n",
    "  - Utilizes Python's `str.format` syntax for templating, supporting any number of variables, including scenarios with no variables.\n",
    "\n",
    "- **ChatPromptTemplate:**\n",
    "  - Designed for chat models, where the prompt consists of a single or list of chat messages.\n",
    "  - Each chat message includes content and a role parameter. For instance, in the OpenAI Chat Completions API, a chat message could be assigned to an AI assistant, a human, or a system role.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAGmjR-tqfmV"
   },
   "source": [
    "##### ChatPromptTemplate with simple prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tawdWLueqHG6"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# more complex prompt with placeholders\n",
    "prompt = \"\"\"Explain to me briefly about {topic} in 1 line.\"\"\"\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_template(prompt)\n",
    "chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQV72oRxwLDJ"
   },
   "source": [
    "Chains enable you to input data dynamically at runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZsCRIH61v6eo"
   },
   "outputs": [],
   "source": [
    "simple_chain = (chat_template\n",
    "                    |\n",
    "                 chatgpt)\n",
    "simple_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8NHhUa48rm_g"
   },
   "outputs": [],
   "source": [
    "topics = ['Generative AI', 'Machine Learning', 'Deep Learning']\n",
    "responses = simple_chain.map().invoke(topics)\n",
    "for response in responses:\n",
    "  print(response.content)\n",
    "  print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWbg_VJ0LwNt"
   },
   "source": [
    "##### ChatPromptTemplate with sequence of prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1PSkafcfsme2"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "        (\"system\", \"Act as an expert in insurance and provide brief answers\"),\n",
    "        (\"human\", \"what is your name?\"),\n",
    "        (\"ai\", \"my name is AIBot\"),\n",
    "        (\"human\", \"{user_prompt}\"),\n",
    "]\n",
    "chat_template = ChatPromptTemplate.from_messages(messages)\n",
    "chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bhOcg9CIwRY8"
   },
   "outputs": [],
   "source": [
    "simple_chain = chat_template | chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N5ncYrVVtUWU"
   },
   "outputs": [],
   "source": [
    "text_prompts = [\"what is your name?\",\n",
    "                \"explain healthcare insurance to me\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73R370WTuAIR"
   },
   "outputs": [],
   "source": [
    "responses = simple_chain.map().invoke(text_prompts)\n",
    "for response in responses:\n",
    "  print(response.content)\n",
    "  print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cei3lVfHduK"
   },
   "source": [
    "## Structured Prompting\n",
    "\n",
    "Output parsers are essential in Langchain for structuring the responses from language models. Below, we will look at how to prompt LLMs to generate structured outouts.\n",
    "\n",
    "![](https://i.imgur.com/qtXFjf3.png)\n",
    "\n",
    "- **Pydantic Schema:**\n",
    "  - This schema allows the specification of an arbitrary Pydantic Model to force LLMs for outputs matching that schema. Pydantic's BaseModel functions similarly to a Python dataclass but includes type checking and coercion.\n",
    "\n",
    "- **Structured Outputs:**\n",
    "  - LangChain provides a method, `with_structured_output()`, that automates the process of binding the schema to the model and parsing the output. This helper function is available for all model providers that support structured output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Td0hVmQd3OVw"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Define your desired data structure - like a python data class.\n",
    "class ITSupportResponse(BaseModel):\n",
    "    orig_msg: str = Field(description=\"The original customer IT support query message\")\n",
    "    orig_lang: str = Field(description=\"Detected language of the customer message e.g. Spanish\")\n",
    "    category: str = Field(description=\"1-2 word describing the category of the problem\")\n",
    "    trans_msg: str = Field(description=\"Translated customer IT support query message in English\")\n",
    "    response: str = Field(description=\"Response to the customer in their original language - orig_lang\")\n",
    "    trans_response: str = Field(description=\"Response to the customer in English\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1HJvSevb4XES"
   },
   "outputs": [],
   "source": [
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "prompt_txt = \"\"\"\n",
    "             Act as an Information Technology (IT) customer support agent. For the IT support message mentioned below\n",
    "             in triple backticks use the given format when generating the output response\n",
    "\n",
    "             Customer IT support message:\n",
    "             ```{it_support_msg}```\n",
    "             \"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate.from_template(template=prompt_txt)\n",
    "prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLQDEvE6i1cn"
   },
   "outputs": [],
   "source": [
    "structured_llm = chatgpt.with_structured_output(ITSupportResponse)\n",
    "structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3dlBUHh7682"
   },
   "outputs": [],
   "source": [
    "it_support_queue = [\n",
    "    \"Não consigo sincronizar meus contatos com o telefone. Sempre recebo uma mensagem de falha.\",\n",
    "    \"Ho problemi a stampare i documenti da remoto. Il lavoro non viene inviato alla stampante di rete.\",\n",
    "    \"プリンターのトナーを交換しましたが、印刷品質が低下しています。サポートが必要です。\",\n",
    "    \"Я не могу войти в систему учета времени, появляется сообщение об ошибке. Мне нужна помощь.\",\n",
    "    \"Internet bağlantım çok yavaş ve bazen tamamen kesiliyor. Yardım eder misiniz?\",\n",
    "    \"Не могу установить обновление безопасности. Появляется код ошибки. Помогите, пожалуйста.\"\n",
    "]\n",
    "\n",
    "formatted_msgs = [{\"it_support_msg\": msg}\n",
    "                    for msg in it_support_queue]\n",
    "formatted_msgs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ySEbc1O0O-Zh"
   },
   "outputs": [],
   "source": [
    "qa_chain = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CnxspdUb8rSd"
   },
   "outputs": [],
   "source": [
    "responses = qa_chain.map().invoke(formatted_msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLhLZnQi8xqq"
   },
   "outputs": [],
   "source": [
    "responses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fejldAZfPJTd"
   },
   "outputs": [],
   "source": [
    "responses[0].model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZNaMZkYPd5X"
   },
   "outputs": [],
   "source": [
    "responses = [response.model_dump() for response in responses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJJMXDsV82Pd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(responses)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "esVD-ghbJCDD"
   },
   "source": [
    "## Streaming in LLMs\n",
    "\n",
    "All language model interfaces (LLMs) in LangChain implement the `Runnable` interface, which provides default methods such as `ainvoke`, `batch`, `abatch`, `stream`, and `astream`. This setup equips all LLMs with basic streaming capabilities.\n",
    "\n",
    "### Streaming Defaults:\n",
    "\n",
    "- **Synchronous Streaming:** By default, streaming operations return an `Iterator` that yields a single value, the final result from the LLM provider.\n",
    "- **Asynchronous Streaming:** Similarly, async streaming defaults to returning an `AsyncIterator` with the final result.\n",
    "\n",
    "### Limitations:\n",
    "\n",
    "- These default implementations do not support token-by-token streaming. For such detailed streaming, the LLM provider must offer native support. However, the default setup ensures that your code expecting an iterator of tokens will function correctly within these constraints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mu7GykqNF7He"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Explain to me what is utlization review in 3 bullets\"\"\"\n",
    "\n",
    "for chunk in chatgpt.stream(prompt):\n",
    "    print(chunk.content, end='')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPMXxDrN5rzm4hqLJNT7oQ0",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dipanjanS/mastering-intelligent-agents-langgraph-workshop-dhs2025/blob/main/Module-1-Introduction-to-Generative-AI-and-Agentic-AI/M1LC5_Build_a_RAG_System_with_LangGraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7d5a5a2",
      "metadata": {
        "id": "e7d5a5a2"
      },
      "source": [
        "\n",
        "# Two-Node LangGraph RAG System (Retrieve → Generate)\n",
        "\n",
        "**Objective:** Build a standard Retrieval-Augmented Generation (RAG) system using **LangGraph** with two nodes:\n",
        "1. **Retrieval Node**: Uses a hybrid retriever to fetch relevant chunks and stores them in `retrieved_docs` in the state.\n",
        "2. **Generation Node**: Formats retrieved docs, runs a RAG prompt with an LLM, and stores the final string answer in `answer`.\n",
        "\n",
        "**Architecture:** `question → retrieve_node → generate_node → answer`\n",
        "\n",
        "![](https://i.imgur.com/FtKPvC8.png)\n",
        "\n",
        "This notebook demonstrates how to use a Retrieval-Augmented Generation (RAG) system to assist operations, audit, and transformation teams in querying past findings and recommendations. The goal is to enable fast, accurate answers based on internal process narratives, helping organizations:\n",
        "\n",
        "- Understand root causes of workflow inefficiencies\n",
        "- Recommend remediation strategies\n",
        "- Identify patterns across departments\n",
        "- Track outcomes of automation interventions (e.g., AutoFlow Insight)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5773dc07",
      "metadata": {
        "id": "5773dc07"
      },
      "source": [
        "## Environment & Dependencies\n",
        "Install libraries and set the API key."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langgraph==0.6.5 langchain==0.3.27 langchain-openai==0.3.29 langchain-community==0.3.27 langchain-chroma==0.2.5 rank-bm25==0.2.2 --quiet"
      ],
      "metadata": {
        "id": "_sICWLx-C4Dj"
      },
      "id": "_sICWLx-C4Dj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter API Keys & Setup Environment Variables"
      ],
      "metadata": {
        "id": "gCiHAEmyxhnl"
      },
      "id": "gCiHAEmyxhnl"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "# OpenAI API Key (for chat & embeddings)\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key (https://platform.openai.com/account/api-keys):\\n\")"
      ],
      "metadata": {
        "id": "E066sqIn1YEJ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "E066sqIn1YEJ"
    },
    {
      "cell_type": "markdown",
      "id": "ea7bb86f",
      "metadata": {
        "id": "ea7bb86f"
      },
      "source": [
        "\n",
        "## Data Loading & Preprocessing\n",
        "\n",
        "We will:\n",
        "- Download the JSONL dataset from Google Drive\n",
        "- Load JSON lines into Python\n",
        "- Build `Document` objects\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1u8ImzhGW2wgIib16Z_wYIaka7sYI_TGK"
      ],
      "metadata": {
        "id": "l23AZNPgHMVb"
      },
      "id": "l23AZNPgHMVb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56b0672e",
      "metadata": {
        "id": "56b0672e"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "# ---- Configure dataset path (update if needed) ----\n",
        "DATA_PATH = Path(\"./rag_demo_docs052025.jsonl\")  # same file name as earlier notebook\n",
        "\n",
        "if not DATA_PATH.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Expected dataset at {DATA_PATH}. \"\n",
        "        \"Please place the JSONL file here or update DATA_PATH.\"\n",
        "    )\n",
        "\n",
        "# Load JSONL\n",
        "raw_docs = []\n",
        "with DATA_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        raw_docs.append(json.loads(line))\n",
        "\n",
        "# Convert to Document objects with metadata\n",
        "documents = []\n",
        "for i, d in enumerate(raw_docs):\n",
        "    sect = d.get(\"sectioned_report\", {})\n",
        "    text = (\n",
        "        f\"Issue:\\n{sect.get('Issue','')}\\n\\n\"\n",
        "        f\"Impact:\\n{sect.get('Impact','')}\\n\\n\"\n",
        "        f\"Root Cause:\\n{sect.get('Root Cause','')}\\n\\n\"\n",
        "        f\"Recommendation:\\n{sect.get('Recommendation','')}\"\n",
        "    )\n",
        "\n",
        "    documents.append(Document(page_content=text))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(documents[0].page_content)"
      ],
      "metadata": {
        "id": "oQ439mGmHy7o"
      },
      "id": "oQ439mGmHy7o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c97997f1",
      "metadata": {
        "id": "c97997f1"
      },
      "source": [
        "\n",
        "## Embeddings & Vector Store (Chroma)\n",
        "\n",
        "- OpenAI `text-embedding-3-small`\n",
        "- Chroma with cosine space\n",
        "- Persist to disk so you can reuse\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf reports_db"
      ],
      "metadata": {
        "id": "CnrKtEzMI5vw"
      },
      "id": "CnrKtEzMI5vw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27e1a42f",
      "metadata": {
        "id": "27e1a42f"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "persist_dir = \"./reports_db\"\n",
        "collection = \"reports_db\"\n",
        "\n",
        "embedder = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# Build or rebuild the vector store\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=documents,\n",
        "    embedding=embedder,\n",
        "    collection_name=collection,\n",
        "    collection_metadata={\"hnsw:space\": \"cosine\"},\n",
        "    persist_directory=persist_dir\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c0114b3",
      "metadata": {
        "id": "3c0114b3"
      },
      "source": [
        "\n",
        "## Retrievers (Hybrid Search)\n",
        "\n",
        "We use the following retrieval strategy:\n",
        "- Semantic similarity (with a score threshold)\n",
        "- BM25 keyword retriever\n",
        "- Ensemble hybrid combination\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reopen handle (demonstrates persistence)\n",
        "vectordb = Chroma(\n",
        "    embedding_function=embedder,\n",
        "    collection_name=collection,\n",
        "    persist_directory=persist_dir,\n",
        ")\n",
        "vectordb._collection.count()"
      ],
      "metadata": {
        "id": "ILjYp5ZhLqv3"
      },
      "id": "ILjYp5ZhLqv3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fd49d93",
      "metadata": {
        "id": "5fd49d93"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "\n",
        "# Base semantic retriever (cosine sim + threshold)\n",
        "semantic = vectordb.as_retriever(\n",
        "    search_type=\"similarity_score_threshold\",\n",
        "    search_kwargs={\"k\": 5, \"score_threshold\": 0.2},\n",
        ")\n",
        "\n",
        "# BM25 keyword retriever\n",
        "bm25 = BM25Retriever.from_documents(documents)\n",
        "bm25.k = 3\n",
        "\n",
        "# Ensemble (hybrid)\n",
        "hybrid_retriever = EnsembleRetriever(\n",
        "    retrievers=[bm25, semantic],\n",
        "    weights=[0.6, 0.4],\n",
        "    k=5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick test\n",
        "hybrid_retriever.invoke(\"What are the major issues in finance approval workflows?\")[:3]"
      ],
      "metadata": {
        "id": "aXTRB773MV9v"
      },
      "id": "aXTRB773MV9v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "51777e6c",
      "metadata": {
        "id": "51777e6c"
      },
      "source": [
        "\n",
        "## LangGraph State Definition\n",
        "\n",
        "We define a simple **state schema** with overwrite behavior (single-turn flow):\n",
        "\n",
        "- `question: str`\n",
        "- `retrieved_docs: list[Document]`\n",
        "- `answer: str`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deb6def3",
      "metadata": {
        "id": "deb6def3"
      },
      "outputs": [],
      "source": [
        "\n",
        "from typing import List, TypedDict, Annotated\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langchain.docstore.document import Document as LCDocument\n",
        "\n",
        "# We keep overwrite semantics for all keys (no reducers needed for appends here).\n",
        "class RAGState(TypedDict):\n",
        "    question: str\n",
        "    retrieved_docs: List[LCDocument]\n",
        "    answer: str\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8897f470",
      "metadata": {
        "id": "8897f470"
      },
      "source": [
        "\n",
        "## Node 1 — Retrieval Node\n",
        "\n",
        "- Reads `state['question']`\n",
        "- Calls `retriever.invoke(question)` (as used in your notebook)\n",
        "- Writes documents into `state['retrieved_docs']`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b57ca9cb",
      "metadata": {
        "id": "b57ca9cb"
      },
      "outputs": [],
      "source": [
        "\n",
        "def retrieve_node(state: RAGState) -> RAGState:\n",
        "    query = state[\"question\"]\n",
        "    docs = hybrid_retriever.invoke(query)  # returns list[Document]\n",
        "    return {\"retrieved_docs\": docs}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd6038e2",
      "metadata": {
        "id": "bd6038e2"
      },
      "source": [
        "\n",
        "## Node 2 — Generation Node (RAG prompt)\n",
        "\n",
        "- Formats retrieved docs into a context string\n",
        "- Uses a grounded prompt: answer only from context, otherwise say you don't know\n",
        "- Stores result text in `state['answer']`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cf4f928",
      "metadata": {
        "id": "7cf4f928"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "PROMPT = ChatPromptTemplate.from_template(\n",
        "    \"\"\"You are an assistant for Analyzing internal reports for Operational Insights.\n",
        "       Use the following pieces of retrieved context to answer the question.\n",
        "       If you don't know the answer or there is no relevant context, just say that you don't know.\n",
        "       give a well-structured and to the point answer using the context information.\n",
        "\n",
        "       Question:\n",
        "       {question}\n",
        "\n",
        "       Context:\n",
        "       {context}\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "def _format_docs(docs: List[LCDocument]) -> str:\n",
        "    return \"\\n\\n\".join(d.page_content for d in docs) if docs else \"\"\n",
        "\n",
        "def generate_node(state: RAGState) -> RAGState:\n",
        "    question = state[\"question\"]\n",
        "    docs = state.get(\"retrieved_docs\", [])\n",
        "    context = _format_docs(docs)\n",
        "    prompt = PROMPT.format(question=question, context=context)\n",
        "    resp = llm.invoke(prompt)\n",
        "    return {\"answer\": resp.content}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef2beea4",
      "metadata": {
        "id": "ef2beea4"
      },
      "source": [
        "\n",
        "## Build the Graph & Edges\n",
        "\n",
        "`START → retrieve_node → generate_node → END`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2460b644",
      "metadata": {
        "id": "2460b644"
      },
      "outputs": [],
      "source": [
        "builder = StateGraph(RAGState)\n",
        "\n",
        "builder.add_node(\"retrieve\", retrieve_node)\n",
        "builder.add_node(\"generate\", generate_node)\n",
        "\n",
        "builder.add_edge(START, \"retrieve\")\n",
        "builder.add_edge(\"retrieve\", \"generate\")\n",
        "builder.add_edge(\"generate\", END)\n",
        "\n",
        "graph = builder.compile()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display, display_markdown\n",
        "\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "sB4A2lJAN6ld"
      },
      "id": "sB4A2lJAN6ld",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "64cdff9d",
      "metadata": {
        "id": "64cdff9d"
      },
      "source": [
        "\n",
        "## Run Examples\n",
        "\n",
        "Invoke with a question and read back the final `answer` from the state.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b1824ae",
      "metadata": {
        "id": "9b1824ae"
      },
      "outputs": [],
      "source": [
        "example_q = \"What are the major issues in finance approval workflows?\"\n",
        "final_state = graph.invoke({\"question\": example_q})\n",
        "final_state"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display_markdown(final_state[\"answer\"], raw=True)"
      ],
      "metadata": {
        "id": "JVKScf7NgV9b"
      },
      "id": "JVKScf7NgV9b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_q = \"What caused invoice SLA breaches in the last quarter?\"\n",
        "final_state = graph.invoke({\"question\": example_q})\n",
        "display_markdown(final_state[\"answer\"], raw=True)"
      ],
      "metadata": {
        "id": "qumCqEQrbDBI"
      },
      "id": "qumCqEQrbDBI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_q = \"How did AutoFlow Insight improve SLA adherence?\"\n",
        "final_state = graph.invoke({\"question\": example_q})\n",
        "display_markdown(final_state[\"answer\"], raw=True)"
      ],
      "metadata": {
        "id": "nFlhfFZlg0QQ"
      },
      "id": "nFlhfFZlg0QQ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
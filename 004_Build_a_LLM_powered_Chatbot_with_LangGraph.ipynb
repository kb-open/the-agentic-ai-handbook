{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dipanjanS/mastering-intelligent-agents-langgraph-workshop-dhs2025/blob/main/Module-1-Introduction-to-Generative-AI-and-Agentic-AI/M1LC4_Build_a_LLM_powered_Chatbot_with_LangGraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50fa7f8a-8764-4bb9-9968-48b681a0e4f1",
      "metadata": {
        "id": "50fa7f8a-8764-4bb9-9968-48b681a0e4f1"
      },
      "source": [
        "# Build a LLM-powered Chatbot in LangGraph\n",
        "\n",
        "LangGraph is not just a framework to create static graphs. We already know that it can be used for building stateful, agentic applications using LLMs.\n",
        "\n",
        "We'll now create a simple LLM-powered chatbot using LangGraph. This chatbot will respond directly to user messages.\n",
        "\n",
        "![](https://i.imgur.com/heeggTe.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff151ef1-fa30-482a-94da-8f49964afbc3",
      "metadata": {
        "id": "ff151ef1-fa30-482a-94da-8f49964afbc3"
      },
      "outputs": [],
      "source": [
        "!pip install langchain==0.3.27 langchain-openai==0.3.29 langgraph==0.6.5 --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter Open AI API Key & Setup Environment Variables"
      ],
      "metadata": {
        "id": "H9c37cLnSrbg"
      },
      "id": "H9c37cLnSrbg"
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "OPENAI_KEY = getpass('Enter Open AI API Key: ')\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_KEY"
      ],
      "metadata": {
        "id": "cv3JzCEx_PAd"
      },
      "execution_count": null,
      "outputs": [],
      "id": "cv3JzCEx_PAd"
    },
    {
      "cell_type": "markdown",
      "id": "5999f8d0-989f-4638-8ade-5c257cbadfe8",
      "metadata": {
        "id": "5999f8d0-989f-4638-8ade-5c257cbadfe8"
      },
      "source": [
        "## State\n",
        "\n",
        "First, define the [State](https://langchain-ai.github.io/langgraph/concepts/low_level/#state) of the graph.\n",
        "\n",
        "The State schema serves as the input schema for all Nodes and Edges in the graph.\n",
        "\n",
        "Let's use the `TypedDict` class from python's `typing` module as our schema, which provides type hints for the keys."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a90709b-ddfa-4671-8acc-c59969a29991",
      "metadata": {
        "id": "6a90709b-ddfa-4671-8acc-c59969a29991"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "888509e1-cbde-4c03-99a0-2560dd2e262d",
      "metadata": {
        "id": "888509e1-cbde-4c03-99a0-2560dd2e262d"
      },
      "source": [
        "## Create the Nodes, Edges and Graph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "def chatbot_node(state: State):\n",
        "    # get the current state (which is basically the input user prompt)\n",
        "    state = state['messages']\n",
        "    llm_response = llm.invoke(state)\n",
        "    return {\"messages\": [llm_response]} # appending llm_response to messages key\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot_node)\n",
        "\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph_builder.add_edge(\"chatbot\", END)\n",
        "\n",
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "IJvHs_Py3uCV"
      },
      "id": "IJvHs_Py3uCV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "JR2L3D5Y3uH9"
      },
      "id": "JR2L3D5Y3uH9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = graph.invoke({\"messages\": \"Explain Agentic AI in 2 bullet points\"})\n",
        "response"
      ],
      "metadata": {
        "id": "Pp5IhgSF3uKA"
      },
      "id": "Pp5IhgSF3uKA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response['messages'][-1].content)"
      ],
      "metadata": {
        "id": "gsaTAPtdKAbo"
      },
      "id": "gsaTAPtdKAbo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display_markdown\n",
        "\n",
        "display_markdown(response['messages'][-1].content, raw=True)"
      ],
      "metadata": {
        "id": "2NbT256yWIMq"
      },
      "id": "2NbT256yWIMq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Invoking vs. Streaming in LangGraph"
      ],
      "metadata": {
        "id": "LmcpBvaFf0TW"
      },
      "id": "LmcpBvaFf0TW"
    },
    {
      "cell_type": "code",
      "source": [
        "response = graph.invoke({\"messages\": \"Explain AI in 1 line\"})\n",
        "print(response['messages'][-1].content)"
      ],
      "metadata": {
        "id": "A3Tc4fJhoL9R"
      },
      "id": "A3Tc4fJhoL9R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = graph.invoke({\"messages\": \"What did we discuss so far?\"})\n",
        "print(response['messages'][-1].content)"
      ],
      "metadata": {
        "id": "gEji1u3MX9OR"
      },
      "id": "gEji1u3MX9OR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db16ab8d-b817-4f3a-befc-a02b579c4fca",
      "metadata": {
        "id": "db16ab8d-b817-4f3a-befc-a02b579c4fca"
      },
      "outputs": [],
      "source": [
        "for event in graph.stream({\"messages\": \"Explain AI in 1 line\"},\n",
        "                          stream_mode='values'):\n",
        "    print(event['messages'])\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dipanjanS/mastering-intelligent-agents-langgraph-workshop-dhs2025/blob/main/Module-5-Building-Agentic-RAG-and-Multimodal-Agentic-AI-Systems/M5LC1_Build_a_Customer_Support_Router_Agentic_RAG_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NffLMDtsJFsY"
      },
      "source": [
        "# Build a Customer Support Router Agentic RAG System\n",
        "\n",
        "In this project, we will leverage the power of AI Agents and RAG Systems to build an intelligent Router Agentic RAG System to handle customer support queries using a custom knowledgebase.\n",
        "\n",
        "![](https://i.imgur.com/Em0lzai.png)\n",
        "\n",
        "### Intelligent Router Agentic RAG System\n",
        "\n",
        "This project focuses on building an **Intelligent Router Agentic RAG System** that combines intelligent query analysis, sentiment detection, and dynamic routing with Retrieval-Augmented Generation (RAG) to handle diverse user inquiries efficiently. The workflow includes the following components:\n",
        "\n",
        "1. **Query Categorization and Sentiment Analysis**:\n",
        "   - The system uses **OpenAI GPT-4o** to analyze the user's query and determine:\n",
        "     - **Query Category**: Identifies the type of problem, such as billing, technical issues, or general queries.\n",
        "     - **User Sentiment**: Evaluates the user's sentiment (positive, neutral, or negative) to determine if escalation is needed.\n",
        "\n",
        "2. **Intelligent Routing**:\n",
        "   - Based on the **query_category** and **query_sentiment**, the system routes the query to the appropriate handling node:\n",
        "     - **Escalate to Human**: If the sentiment is negative, the query is escalated to a human for resolution.\n",
        "     - **Generate Billing Response**: Queries related to billing are routed to generate an appropriate response.\n",
        "     - **Generate Technical Response**: Technical queries are routed for a specialized technical response.\n",
        "     - **Generate General Response**: General queries are handled with context-aware responses.\n",
        "\n",
        "3. **Knowledge Base Integration (RAG)**:\n",
        "   - The system integrates with a **Knowledge Base (Vector Database)** to augment responses with relevant and accurate information.\n",
        "   - Retrieval-Augmented Generation (RAG) ensures that responses are grounded in the latest and most reliable data.\n",
        "\n",
        "4. **Escalation Mechanism**:\n",
        "   - Negative sentiment triggers an **escalation to a human**, ensuring the user receives empathetic and personalized support for critical issues.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install OpenAI, LangGraph and LangChain dependencies"
      ],
      "metadata": {
        "id": "9hEI3WL328vZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.3.27 langchain-community==0.3.27 langchain-openai==0.3.30 langgraph==0.6.5 --quiet"
      ],
      "metadata": {
        "id": "dzjE7G_8KOUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-chroma==0.2.5 --quiet"
      ],
      "metadata": {
        "id": "f64P4sY6RtNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipywidgets jupyter-ui-poll==1.0.0 --quiet"
      ],
      "metadata": {
        "id": "5hBVMB7BE-Vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter API Keys & Setup Environment Variables"
      ],
      "metadata": {
        "id": "2Q_pwVLQv4Vj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "# OpenAI API Key (for chat & embeddings)\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key (https://platform.openai.com/account/api-keys):\\n\")"
      ],
      "metadata": {
        "id": "WI-dm80vv4Vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Company Knowledge Base"
      ],
      "metadata": {
        "id": "iLVgrOIgR_Z1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# or download manually from https://drive.google.com/file/d/1CWHutosAcJ6fiddQW5ogvg7NgLstZJ9j/view?usp=sharing and upload to colab or your notebook location\n",
        "!gdown 1CWHutosAcJ6fiddQW5ogvg7NgLstZJ9j"
      ],
      "metadata": {
        "id": "Mjlzx9bxR-1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"./router_agent_documents.json\", \"r\") as f:\n",
        "    knowledge_base = json.load(f)"
      ],
      "metadata": {
        "id": "7NzAAMXyTSAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.docstore.document import Document\n",
        "from tqdm import tqdm\n",
        "\n",
        "processed_docs = []\n",
        "\n",
        "for doc in tqdm(knowledge_base):\n",
        "    metadata = doc['metadata']\n",
        "    data = doc['text']\n",
        "    processed_docs.append(Document(page_content=data,\n",
        "                                   metadata=metadata))\n",
        "\n",
        "processed_docs[:3]"
      ],
      "metadata": {
        "id": "2ne7fLkRTdiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_docs[0]"
      ],
      "metadata": {
        "id": "SH5gdZaqkd9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categories = set([doc.metadata['category'] for doc in processed_docs])\n",
        "categories"
      ],
      "metadata": {
        "id": "sIh_wqgGFhET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "billing_data = [doc for doc in processed_docs if doc.metadata['category'] == 'billing']\n",
        "technical_data = [doc for doc in processed_docs if doc.metadata['category'] == 'technical']\n",
        "general_data = [doc for doc in processed_docs if doc.metadata['category'] == 'general']"
      ],
      "metadata": {
        "id": "dXnsUTh1FuAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Vector Databases"
      ],
      "metadata": {
        "id": "M-NDea2_HAZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# details here: https://openai.com/blog/new-embedding-models-and-api-updates\n",
        "openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-small')"
      ],
      "metadata": {
        "id": "f3Ngz9vqUGt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "billing_db = Chroma.from_documents(documents=billing_data,\n",
        "                                   collection_name='billing_database',\n",
        "                                   embedding=openai_embed_model,\n",
        "                                   # need to set the distance function to cosine else it uses euclidean by default\n",
        "                                   # check https://docs.trychroma.com/guides#changing-the-distance-function\n",
        "                                   collection_metadata={\"hnsw:space\": \"cosine\"},\n",
        "                                   persist_directory=\"./knowledge_bases\")\n",
        "\n",
        "technical_db = Chroma.from_documents(documents=technical_data,\n",
        "                                     collection_name='technical_database',\n",
        "                                     embedding=openai_embed_model,\n",
        "                                     collection_metadata={\"hnsw:space\": \"cosine\"},\n",
        "                                     persist_directory=\"./knowledge_bases\")\n",
        "\n",
        "general_db = Chroma.from_documents(documents=general_data,\n",
        "                                   collection_name='general_database',\n",
        "                                   embedding=openai_embed_model,\n",
        "                                   collection_metadata={\"hnsw:space\": \"cosine\"},\n",
        "                                   persist_directory=\"./knowledge_bases\")"
      ],
      "metadata": {
        "id": "g6C6d4AkUVhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "billing_retriever = billing_db.as_retriever(search_type=\"similarity_score_threshold\",\n",
        "                                            search_kwargs={\"k\": 3, \"score_threshold\": 0.2})\n",
        "\n",
        "techical_retriever = technical_db.as_retriever(search_type=\"similarity_score_threshold\",\n",
        "                                               search_kwargs={\"k\": 3, \"score_threshold\": 0.2})\n",
        "\n",
        "general_retriever = general_db.as_retriever(search_type=\"similarity_score_threshold\",\n",
        "                                            search_kwargs={\"k\": 3, \"score_threshold\": 0.2})"
      ],
      "metadata": {
        "id": "-Wppf1mhUw0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'what is your refund policy?'\n",
        "billing_retriever.invoke(query)"
      ],
      "metadata": {
        "id": "ryudORhuUNDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Um1WrTcJFsl"
      },
      "source": [
        "## Define the Agent State Schema\n",
        "\n",
        "We create a `CustomerSupportState` typed dictionary to keep track of each interaction:\n",
        "- **customer_query**: The text of the customer's question\n",
        "- **query_category**: Technical, Billing, or General (used for routing)\n",
        "- **query_sentiment**: Positive, Neutral, or Negative (used for routing)\n",
        "- **final_response**: The system's response to the customer\n",
        "- **escalation_cust_info**: The customer details when input in the escalation form"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2oH7LzqJFsn"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from typing import TypedDict, Literal\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class CustomerSupportState(TypedDict):\n",
        "    customer_query: str\n",
        "    query_category: str\n",
        "    query_sentiment: str\n",
        "    final_response: str\n",
        "    escalation_cust_info: dict\n",
        "\n",
        "class QueryCategory(BaseModel):\n",
        "    categorized_topic: Literal['Technical', 'Billing', 'General']\n",
        "\n",
        "class QuerySentiment(BaseModel):\n",
        "    sentiment: Literal['Positive', 'Neutral', 'Negative']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plan the Agent Workflow Structure\n",
        "\n",
        "This is the Agent workflow we will be using\n",
        "\n",
        "![](https://i.imgur.com/tyXpJ4J.png)\n",
        "\n",
        "Next up we will define python functions for each of the nodes in this Agent graph"
      ],
      "metadata": {
        "id": "FJkk-V19PUQB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL-RUuX-JFso"
      },
      "source": [
        "## Create Node Functions\n",
        "\n",
        "Each function below represents a stage in processing a customer inquiry:\n",
        "\n",
        "1. **categorize_inquiry**: Classifies the query into Technical, Billing, or General.\n",
        "2. **analyze_inquiry_sentiment**: Determines if the sentiment is Positive, Neutral, or Negative.\n",
        "3. **generate_technical_response**: Produces a response for technical issues.\n",
        "4. **generate_billing_response**: Produces a response for billing questions.\n",
        "5. **generate_general_response**: Produces a response for general queries.\n",
        "6. **accept_user_input_escalation**: Collects additional input from the user when a negative sentiment is detected to facilitate escalation to a human support agent.\n",
        "6. **escalate_to_human_agent**: Escalates the query and submitted form details to a human agent for personalized resolution.\n",
        "7. **determine_route**: Routes the inquiry to the appropriate response node based on category and sentiment."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
      ],
      "metadata": {
        "id": "XqvrHu8ZX2tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the categorize_inquiry node function\n",
        "\n",
        "Helps in classifying the user query to either Technical, Billing, or General."
      ],
      "metadata": {
        "id": "3nfTUwuZI1BR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IktZ07zDJFsp"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def categorize_inquiry(support_state: CustomerSupportState) -> CustomerSupportState:\n",
        "    \"\"\"\n",
        "    Classify the customer query into Technical, Billing, or General.\n",
        "    \"\"\"\n",
        "\n",
        "    query = support_state[\"customer_query\"]\n",
        "    ROUTE_CATEGORY_PROMPT = \"\"\"Act as a customer support agent trying to best categorize the customer query.\n",
        "                               You are an agent for an AI products and hardware company.\n",
        "\n",
        "                               Please read the customer query below and\n",
        "                               determine the best category from the following list:\n",
        "\n",
        "                               'Technical', 'Billing', or 'General'.\n",
        "\n",
        "                               Remember:\n",
        "                                - Technical queries will focus more on technical aspects like AI models, hardware, software related queries etc.\n",
        "                                - General queries will focus more on general aspects like contacting support, finding things, policies etc.\n",
        "                                - Billing queries will focus more on payment and purchase related aspects\n",
        "\n",
        "                                Return just the category name (from one of the above)\n",
        "\n",
        "                                Query:\n",
        "                                {customer_query}\n",
        "                            \"\"\"\n",
        "    prompt = ROUTE_CATEGORY_PROMPT.format(customer_query=query)\n",
        "    route_category = llm.with_structured_output(QueryCategory).invoke(prompt)\n",
        "\n",
        "    return {\n",
        "        \"query_category\": route_category.categorized_topic\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "categorize_inquiry({\"customer_query\": \"Do you provide pretrained models?\"})"
      ],
      "metadata": {
        "id": "Uh2L4b4YaBVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorize_inquiry({\"customer_query\": \"what payment methods are accepted?\"})"
      ],
      "metadata": {
        "id": "Pa3vyedQbxin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the analyze_inquiry_sentiment node function\n",
        "\n",
        "Helps in classifying the user query to either Positive, Neutral, or Negative based on the sentiment."
      ],
      "metadata": {
        "id": "ghMRpOW1I9DR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_inquiry_sentiment(support_state: CustomerSupportState) -> CustomerSupportState:\n",
        "    \"\"\"\n",
        "    Analyze the sentiment of the customer query as Positive, Neutral, or Negative.\n",
        "    \"\"\"\n",
        "\n",
        "    query = support_state[\"customer_query\"]\n",
        "    SENTIMENT_CATEGORY_PROMPT = \"\"\"Act as a customer support agent trying to best categorize the customer query's sentiment.\n",
        "                                   You are an agent for an AI products and hardware company.\n",
        "\n",
        "                                   Please read the customer query below,\n",
        "                                   analyze its sentiment which should be one from the following list:\n",
        "\n",
        "                                   'Positive', 'Neutral', or 'Negative'.\n",
        "\n",
        "                                   Return just the sentiment (from one of the above)\n",
        "\n",
        "                                   Query:\n",
        "                                   {customer_query}\n",
        "                                \"\"\"\n",
        "    prompt = SENTIMENT_CATEGORY_PROMPT.format(customer_query=query)\n",
        "    sentiment_category = llm.with_structured_output(QuerySentiment).invoke(prompt)\n",
        "\n",
        "    return {\n",
        "        \"query_sentiment\": sentiment_category.sentiment\n",
        "    }"
      ],
      "metadata": {
        "id": "8xoHBg4HZ-tV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_inquiry_sentiment({\"customer_query\": \"what is your refund policy?\"})"
      ],
      "metadata": {
        "id": "_ByyxDpxd3Ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_inquiry_sentiment({\"customer_query\": \"what is your refund policy? I am really fed up with this product and need to refund it\"})"
      ],
      "metadata": {
        "id": "V2Q9rU7md6mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the generate_technical_response node function\n",
        "\n",
        "Provide a technical support response to the user query by combining knowledge from the vector store and LLM using RAG"
      ],
      "metadata": {
        "id": "A3J3q3DwJMxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from typing import Dict\n",
        "\n",
        "def generate_technical_response(support_state: CustomerSupportState) -> CustomerSupportState:\n",
        "    \"\"\"\n",
        "    Provide a technical support response by combining knowledge from the vector store and LLM.\n",
        "    \"\"\"\n",
        "    query = support_state[\"customer_query\"]\n",
        "\n",
        "    # Perform retrieval from VectorDB\n",
        "    relevant_docs = techical_retriever.invoke(query)\n",
        "    retrieved_content = \"\\n\\n\".join(doc.page_content for doc in relevant_docs)\n",
        "\n",
        "    # Combine retrieved information into the prompt\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"\"\"\n",
        "        Craft a clear and detailed technical support response for the following customer query.\n",
        "        Use the provided knowledge base information to enrich your response.\n",
        "        Provide just the response content - do not add any boilerplate template information.\n",
        "        In case there is no knowledge base information or you do not know the answer just say:\n",
        "\n",
        "        Apologies I was not able to answer your question, please reach out to +1-xxx-xxxx\n",
        "\n",
        "        Customer Query:\n",
        "        {customer_query}\n",
        "\n",
        "        Relevant Knowledge Base Information:\n",
        "        {retrieved_content}\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    # Generate the final response using the LLM\n",
        "    chain = prompt | llm\n",
        "    tech_reply = chain.invoke({\n",
        "        \"customer_query\": query,\n",
        "        \"retrieved_content\": retrieved_content\n",
        "    }).content\n",
        "\n",
        "    # Update and return the modified support state\n",
        "    return {\n",
        "        \"final_response\": tech_reply\n",
        "    }\n"
      ],
      "metadata": {
        "id": "rgss3fymfknr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_technical_response({\"customer_query\": \"do you support on-prem models?\",\n",
        "                             \"query_category\": \"Technical\"})"
      ],
      "metadata": {
        "id": "RfrobVESgoA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_technical_response({\"customer_query\": \"what is your refund policy?\", \"query_category\": \"Technical\"})"
      ],
      "metadata": {
        "id": "AVXPLD2Zgf-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the generate_billing_response node function\n",
        "\n",
        "Provide a billing support response to the user query by combining knowledge from the vector store and LLM using RAG"
      ],
      "metadata": {
        "id": "u9LNUK8_JTU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_billing_response(support_state: CustomerSupportState) -> CustomerSupportState:\n",
        "    \"\"\"\n",
        "    Provide a billing support response by combining knowledge from the vector store and LLM.\n",
        "    \"\"\"\n",
        "    query = support_state[\"customer_query\"]\n",
        "\n",
        "    # Perform retrieval from VectorDB\n",
        "    relevant_docs = billing_retriever.invoke(query)\n",
        "    retrieved_content = \"\\n\\n\".join(doc.page_content for doc in relevant_docs)\n",
        "\n",
        "    # Combine retrieved information into the prompt\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"\"\"\n",
        "        Craft a clear and detailed billing support response for the following customer query.\n",
        "        Use the provided knowledge base information to enrich your response.\n",
        "        Provide just the response content - do not add any boilerplate template information.\n",
        "        In case there is no knowledge base information or you do not know the answer just say:\n",
        "\n",
        "        Apologies I was not able to answer your question, please reach out to +1-xxx-xxxx\n",
        "\n",
        "        Customer Query:\n",
        "        {customer_query}\n",
        "\n",
        "        Relevant Knowledge Base Information:\n",
        "        {retrieved_content}\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    # Generate the final response using the LLM\n",
        "    chain = prompt | llm\n",
        "    billing_reply = chain.invoke({\n",
        "        \"customer_query\": query,\n",
        "        \"retrieved_content\": retrieved_content\n",
        "    }).content\n",
        "\n",
        "    # Update and return the modified support state\n",
        "    return {\n",
        "        \"final_response\": billing_reply\n",
        "    }"
      ],
      "metadata": {
        "id": "Ex9O4DW7iR7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the generate_general_response node function\n",
        "\n",
        "Provide a general support response to the user query by combining knowledge from the vector store and LLM using RAG"
      ],
      "metadata": {
        "id": "vTE2T-OjJYDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_general_response(support_state: CustomerSupportState) -> CustomerSupportState:\n",
        "    \"\"\"\n",
        "    Provide a general support response by combining knowledge from the vector store and LLM.\n",
        "    \"\"\"\n",
        "    query = support_state[\"customer_query\"]\n",
        "\n",
        "    # Perform retrieval from VectorDB\n",
        "    relevant_docs = general_retriever.invoke(query)\n",
        "    retrieved_content = \"\\n\\n\".join(doc.page_content for doc in relevant_docs)\n",
        "\n",
        "    # Combine retrieved information into the prompt\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"\"\"\n",
        "        Craft a clear and detailed general support response for the following customer query.\n",
        "        Use the provided knowledge base information to enrich your response.\n",
        "        Provide just the response content - do not add any boilerplate template information.\n",
        "        In case there is no knowledge base information or you do not know the answer just say:\n",
        "\n",
        "        Apologies I was not able to answer your question, please reach out to +1-xxx-xxxx\n",
        "\n",
        "        Customer Query:\n",
        "        {customer_query}\n",
        "\n",
        "        Relevant Knowledge Base Information:\n",
        "        {retrieved_content}\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    # Generate the final response using the LLM\n",
        "    chain = prompt | llm\n",
        "    general_reply = chain.invoke({\n",
        "        \"customer_query\": query,\n",
        "        \"retrieved_content\": retrieved_content\n",
        "    }).content\n",
        "\n",
        "    # Update and return the modified support state\n",
        "    return {\n",
        "        \"final_response\": general_reply\n",
        "    }\n"
      ],
      "metadata": {
        "id": "xTS_38PGjiKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the accept_user_input_escalation function\n",
        "\n",
        "![](https://i.imgur.com/nwTR2ce.png)\n",
        "\n",
        "Collects additional input from the user when a negative sentiment is detected to facilitate escalation to a human support agent.\n",
        "\n",
        "This function opens up a simple form using Jupyter widgets as a User Interface (UI), accepts information from the user in the UI and saves it in the agent state to be used later. Example shown below here:\n",
        "\n",
        "![](https://i.imgur.com/feZqjwj.png)\n",
        "\n",
        "__Note:__ You are free to implement your own UI based workflows to accept user input when you build or integrate such a system in your existing applications \\ organization"
      ],
      "metadata": {
        "id": "hmViwDnHJogv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from jupyter_ui_poll import ui_events\n",
        "import time\n",
        "\n",
        "def accept_user_input_escalation(support_state: CustomerSupportState) -> CustomerSupportState:\n",
        "\n",
        "    # REMEMBER: You can always customize the way you accept user input by modifying the code below\n",
        "    # here we use jupyter widgets so you don't have to install too many external dependencies\n",
        "\n",
        "    global form_submitted  # status variable to track form submission\n",
        "    form_submitted = False # initially form hasn't been submitted\n",
        "\n",
        "    # UI Header: A simple HTML element to label the form\n",
        "    header = widgets.HTML(\"<h3>Escalation Form - Please enter your details below:</h3>\")\n",
        "\n",
        "    # Text input fields to collect user information\n",
        "    input1 = widgets.Text(description='Name:')     # User's full name\n",
        "    input2 = widgets.Text(description='Number:')   # Contact number\n",
        "    input3 = widgets.Text(description='Email:')    # Email address\n",
        "\n",
        "    # Dictionary to store form responses after submission\n",
        "    result = {}\n",
        "\n",
        "    # Callback function to be triggered when the Submit button is clicked\n",
        "    def on_submit(submit_button):\n",
        "        global form_submitted\n",
        "        form_submitted = True  # Mark the form as submitted\n",
        "        # Store user inputs into the result dictionary\n",
        "        result['name'] = input1.value\n",
        "        result['number'] = input2.value\n",
        "        result['email'] = input3.value\n",
        "        # Provide visual feedback that form is submitted\n",
        "        submit_button.description = '👍'\n",
        "\n",
        "    # Submit button widget setup\n",
        "    submit_button = widgets.Button(description=\"Submit\")\n",
        "    submit_button.on_click(on_submit)  # Attach callback to button\n",
        "\n",
        "    # Combine all widgets into a vertical layout box\n",
        "    vbox = widgets.VBox([header, input1, input2, input3, submit_button])\n",
        "    display(vbox)  # Render the form in the notebook interface\n",
        "\n",
        "    # Keep polling UI events until the form is submitted\n",
        "    with ui_events() as poll:\n",
        "        while form_submitted is False:\n",
        "            poll(5)               # Listen for UI events\n",
        "            print('.', end='')   # Show a dot to indicate waiting for input from user\n",
        "            time.sleep(0.3)      # Slight delay to reduce CPU usage\n",
        "\n",
        "    # Return updated agent state with captured user info for escalation\n",
        "    return {\n",
        "        'escalation_cust_info': result\n",
        "    }\n"
      ],
      "metadata": {
        "id": "FGYbKabbJ5kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accept_user_input_escalation({})"
      ],
      "metadata": {
        "id": "8Ot9VBTyJ-M0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the escalate_to_human_agent function\n",
        "\n",
        "Escalates the query and submitted form details to a human agent for personalized resolution.\n",
        "\n",
        "The agent shows a response back to the user with their form input details that they will be contacted soon.\n",
        "\n",
        "__Note:__ You are free to implement any custom logic here to:\n",
        " - Send emails to human agent\n",
        " - Call APIs to notify support agent like Whatsapp or internal company platforms"
      ],
      "metadata": {
        "id": "OFa4IKEGKFUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def escalate_to_human_agent(support_state: CustomerSupportState) -> CustomerSupportState:\n",
        "\n",
        "    # REMEMBER: You can always customize the way you notify the human support agent by adding custom code below.\n",
        "    # This could include emailing them, paging them, sending them notifications using specific platform APIs like whatsapp etc.\n",
        "    # Here we have kept it very simple:\n",
        "    #  we just show a response back to the user showing the details they entered in the form earlier\n",
        "    #  and telling them they will be contacted by a human support agent\n",
        "\n",
        "    # get the customer info from agent state which they entered in the form\n",
        "    escalation_cust_info = support_state['escalation_cust_info']\n",
        "    # the following response will be shown to the user and can also be sent (customer form inputs) to your human support agents\n",
        "    response = (\"Apologies, \" + escalation_cust_info['name'] +\n",
        "                \",  we are really sorry! Someone from our team will be reaching out to via email shortly at \"+\n",
        "                escalation_cust_info['email'] + \" and if needed we will also be calling you at: \" +\n",
        "                escalation_cust_info['number'] + \" to help you out!\")\n",
        "\n",
        "    # NOTE: You can always add custom code here to call specific APIs like whatsapp or email etc to notify your human support agents\n",
        "\n",
        "    return {\n",
        "        \"final_response\": response\n",
        "    }"
      ],
      "metadata": {
        "id": "udRwW-sscAwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QahrWKv3JFsq"
      },
      "source": [
        "## Build and Compile the Agent Workflow\n",
        "\n",
        "We construct a LangGraph Agentic workflow with the nodes defined above:\n",
        "1. **categorize_inquiry** → **analyze_inquiry_sentiment** → **route** to the proper response node.\n",
        "2. If negative, escalate to a human agent by filling in the customer escalation form first.\n",
        "3. Otherwise, produce an appropriate response (technical, billing, or general)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmMiVOgqJFsr"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# Create the graph with our typed state\n",
        "customer_support_graph = StateGraph(CustomerSupportState)\n",
        "\n",
        "# Add nodes for each function\n",
        "customer_support_graph.add_node(\"categorize_inquiry\", categorize_inquiry)\n",
        "customer_support_graph.add_node(\"analyze_inquiry_sentiment\", analyze_inquiry_sentiment)\n",
        "customer_support_graph.add_node(\"generate_technical_response\", generate_technical_response)\n",
        "customer_support_graph.add_node(\"generate_billing_response\", generate_billing_response)\n",
        "customer_support_graph.add_node(\"generate_general_response\", generate_general_response)\n",
        "customer_support_graph.add_node(\"accept_user_input_escalation\", accept_user_input_escalation)\n",
        "customer_support_graph.add_node(\"escalate_to_human_agent\", escalate_to_human_agent)\n",
        "\n",
        "# Add edges to represent the processing flow\n",
        "customer_support_graph.add_edge(\"categorize_inquiry\", \"analyze_inquiry_sentiment\")\n",
        "\n",
        "# routing function\n",
        "def determine_route(support_state: CustomerSupportState) -> str:\n",
        "    \"\"\"\n",
        "    Route the inquiry based on sentiment and category.\n",
        "    \"\"\"\n",
        "    if support_state[\"query_sentiment\"] == \"Negative\":\n",
        "        return \"human_escalate_route\"\n",
        "    elif support_state[\"query_category\"] == \"Technical\":\n",
        "        return \"technical_rag_route\"\n",
        "    elif support_state[\"query_category\"] == \"Billing\":\n",
        "        return \"billing_rag_route\"\n",
        "    else:\n",
        "        return \"general_rag_route\"\n",
        "\n",
        "customer_support_graph.add_conditional_edges(\n",
        "    \"analyze_inquiry_sentiment\",\n",
        "    determine_route,\n",
        "    {\n",
        "        \"human_escalate_route\": \"accept_user_input_escalation\",\n",
        "        \"technical_rag_route\": \"generate_technical_response\",\n",
        "        \"billing_rag_route\": \"generate_billing_response\",\n",
        "        \"general_rag_route\": \"generate_general_response\"\n",
        "    }\n",
        ")\n",
        "\n",
        "customer_support_graph.add_edge(\"accept_user_input_escalation\", \"escalate_to_human_agent\")\n",
        "\n",
        "# All terminal nodes lead to the END\n",
        "customer_support_graph.add_edge(\"generate_technical_response\", END)\n",
        "customer_support_graph.add_edge(\"generate_billing_response\", END)\n",
        "customer_support_graph.add_edge(\"generate_general_response\", END)\n",
        "customer_support_graph.add_edge(\"escalate_to_human_agent\", END)\n",
        "\n",
        "# Set the entry point for the workflow\n",
        "customer_support_graph.set_entry_point(\"categorize_inquiry\")\n",
        "\n",
        "# Compile the graph into a runnable agent\n",
        "memory = MemorySaver()\n",
        "form_submitted = False\n",
        "compiled_support_agent = customer_support_graph.compile(checkpointer=memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Txzt11uTJFss"
      },
      "source": [
        "## Visualize the Agent Workflow\n",
        "\n",
        "Below is a generated diagram of the workflow using Mermaid syntax. It shows how each node connects in the graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qt89sV4kJFst"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from IPython.display import display, Image, Markdown\n",
        "\n",
        "display(Image(compiled_support_agent.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVfgP2P9JFsv"
      },
      "source": [
        "## Helper Function to Run the Agent Workflow\n",
        "\n",
        "This function takes a customer query and runs it through our compiled workflow, returning the final results (category, sentiment, and generated response)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O409_2mRJFsw"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def call_support_agent(agent, prompt, user_session_id, verbose=False):\n",
        "    events = agent.stream(\n",
        "        {\"customer_query\": prompt}, # initial state of the agent\n",
        "        {\"configurable\": {\"thread_id\": user_session_id}},\n",
        "        stream_mode=\"values\",\n",
        "    )\n",
        "\n",
        "    print('Running Agent. Please wait...')\n",
        "    for event in events:\n",
        "        if verbose:\n",
        "                print(event)\n",
        "\n",
        "    print('\\nAgent Response:')\n",
        "    display(Markdown(event['final_response'].replace(r'$', r'\\$')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VeWUyGNJFsw"
      },
      "source": [
        "## Testing the Customer Support Agent Workflow\n",
        "\n",
        "Let's test the workflow with some sample queries to verify categorization, sentiment analysis, and response generation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Image, Markdown"
      ],
      "metadata": {
        "id": "fO3mNApCtS0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uid = 'jim001'\n",
        "query = \"do you support pre-trained models?\"\n",
        "call_support_agent(agent=compiled_support_agent,\n",
        "                   prompt=query,\n",
        "                   user_session_id=uid,\n",
        "                   verbose=True)"
      ],
      "metadata": {
        "id": "s6v7AAnFlp8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4624wCxJFsx"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "query = \"how do I get my invoice?\"\n",
        "call_support_agent(agent=compiled_support_agent,\n",
        "                   prompt=query,\n",
        "                   user_session_id=uid,\n",
        "                   verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Can you tell me about your shipping policy?\"\n",
        "call_support_agent(agent=compiled_support_agent,\n",
        "                   prompt=query,\n",
        "                   user_session_id=uid,\n",
        "                   verbose=False)"
      ],
      "metadata": {
        "id": "LIPGRwMJn5-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"I'm fed up with this faulty hardware, I need a refund\"\n",
        "call_support_agent(agent=compiled_support_agent,\n",
        "                   prompt=query,\n",
        "                   user_session_id=uid,\n",
        "                   verbose=True)"
      ],
      "metadata": {
        "id": "FNspMHcJoBBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are your working hours?\"\n",
        "call_support_agent(agent=compiled_support_agent,\n",
        "                   prompt=query,\n",
        "                   user_session_id=uid,\n",
        "                   verbose=True)"
      ],
      "metadata": {
        "id": "BMzoQZsBNY9n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
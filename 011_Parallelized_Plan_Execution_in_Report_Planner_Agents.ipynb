{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dipanjanS/mastering-intelligent-agents-langgraph-workshop-dhs2025/blob/main/Module-4-Building-Advanced-Agentic-AI-Systems/M4LC1_Parallelized_Plan_Execution_in_Report_Planner_Agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4169bfb-769a-4db3-833e-c827f19024b2",
      "metadata": {
        "id": "f4169bfb-769a-4db3-833e-c827f19024b2"
      },
      "source": [
        "# Parallelized Plan Execution in Planning Agents with  Map-Reduce using Send in LangGraph\n",
        "\n",
        "\n",
        "Map-reduce operations are essential for efficient task decomposition and parallel processing.\n",
        "\n",
        "It has two phases:\n",
        "\n",
        "(1) `Map` - Break a task into smaller sub-tasks, processing each sub-task in parallel.\n",
        "\n",
        "(2) `Reduce` - Aggregate the results across all of the completed, parallelized sub-tasks.\n",
        "\n",
        "We will design a system that will do two things:\n",
        "\n",
        "(1) `Map` - Create a set of questions about a topic. Then answer them in parallel (using `Send(...)`)\n",
        "\n",
        "(2) `Reduce` - Compile a comprehensive report based on the QAs on the topic.\n",
        "\n",
        "![](https://i.imgur.com/SN7KifO.png)\n",
        "\n",
        "\n",
        "LangGraph's Map-Reduce pattern enables efficient task decomposition and parallel processing, enhancing performance in complex workflows. The Send function plays a pivotal role in this mechanism.\n",
        "\n",
        "Map-Reduce in LangGraph:\n",
        "- Task Decomposition: Breaks down a large task into smaller, manageable sub-tasks (planning or complex question decomposition)\n",
        "- Parallel Processing: Executes sub-tasks concurrently, significantly reducing overall processing time.\n",
        "- Result Aggregation: Combines outcomes from all sub-tasks to form a comprehensive response.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install OpenAI, LangGraph and LangChain dependencies"
      ],
      "metadata": {
        "id": "9hEI3WL328vZ"
      },
      "id": "9hEI3WL328vZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "618eab5c-4ef7-4273-8e0b-a9c847897ed7",
      "metadata": {
        "id": "618eab5c-4ef7-4273-8e0b-a9c847897ed7"
      },
      "outputs": [],
      "source": [
        "!pip install langchain==0.3.27 langchain-community==0.3.27 langchain-openai==0.3.30 langgraph==0.6.5 --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter Open AI API Key & Setup Environment Variables"
      ],
      "metadata": {
        "id": "H9c37cLnSrbg"
      },
      "id": "H9c37cLnSrbg"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "# OpenAI API Key (for chat & embeddings)\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key (https://platform.openai.com/account/api-keys):\\n\")\n"
      ],
      "metadata": {
        "id": "cv3JzCEx_PAd"
      },
      "execution_count": null,
      "outputs": [],
      "id": "cv3JzCEx_PAd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Agent State Schema"
      ],
      "metadata": {
        "id": "E1OZCIkyYYzt"
      },
      "id": "E1OZCIkyYYzt"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypedDict\n",
        "from pydantic import BaseModel\n",
        "import operator\n",
        "from typing import Annotated\n",
        "\n",
        "# Define state\n",
        "class Questions(BaseModel):\n",
        "    questions: list[str]\n",
        "\n",
        "class Answer(BaseModel):\n",
        "    question: str\n",
        "    answer: str\n",
        "\n",
        "class Report(BaseModel):\n",
        "    report: str\n",
        "\n",
        "class OverallState(TypedDict):\n",
        "    topic: str\n",
        "    questions: list\n",
        "    answers: Annotated[list, operator.add]\n",
        "    report: str"
      ],
      "metadata": {
        "id": "o7EnucYkRb6f"
      },
      "id": "o7EnucYkRb6f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Agent Node Functions\n",
        "\n",
        "Role of the Send Function:\n",
        "- Dynamic Task Distribution: Utilizes the Send function to dispatch different states to multiple instances of a node, facilitating parallel execution.\n",
        "- Flexible Workflow Management: Very useful when you do not have a fixed number of static edges to parallelize like in router agent. A simple example would be generating a random number of questions or steps to solve a problem and parallelizing the generation process for each of those questions or steps."
      ],
      "metadata": {
        "id": "8UUj4MMIYfuH"
      },
      "id": "8UUj4MMIYfuH"
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.types import Send\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# Node to generate questions\n",
        "def generate_questions(state: OverallState):\n",
        "    # sometimes gpt-4o just generates 5 questions always so feel free to play around with the following prompt\n",
        "    # or you can also use gpt-4o-mini or other LLMs or just randomly select a subselt of questions from the response\n",
        "    # just to demonstrate and see how Send works with a variable number of questions\n",
        "    questions_prompt = \"\"\"Generate a list of concise sub-questions related to this overall topic: {topic}\n",
        "                          which would help build a good report.\n",
        "                          Follow these rules for question generation:\n",
        "                            - Do not create very long questions.\n",
        "                            - Number of questions should always be 3 for broad generic topics (Birds, Animals, Artifical Intelligence)\n",
        "                              and 5 for more complex specific topics (Outlook for ..., Impact of ...)\n",
        "                       \"\"\"\n",
        "    prompt = questions_prompt.format(topic=state[\"topic\"])\n",
        "    response = llm.with_structured_output(Questions).invoke(prompt)\n",
        "    return {\"questions\": response.questions}\n",
        "\n",
        "# Node to generate answer to one question\n",
        "def generate_answer(state: Answer):\n",
        "    answer_prompt = \"\"\"Generate the answer about {question}.\"\"\"\n",
        "    prompt = answer_prompt.format(question=state[\"question\"])\n",
        "    response = llm.with_structured_output(Answer).invoke(prompt)\n",
        "    return {\"answers\": [{\"question\": state[\"question\"], \"answer\": response.answer}]}\n",
        "\n",
        "# Node to parallelize answer generation\n",
        "def parallelize_answer_generation(state: OverallState):\n",
        "    return [Send(\"generate_answer\", {\"question\": q}) for q in state[\"questions\"]] # does the parallel execution\n",
        "\n",
        "# Node to compile the report\n",
        "def compile_report(state: OverallState):\n",
        "    q_and_a = \"\\n\\n\".join(\n",
        "        [f\"Q: {qa['question']}\\nA: {qa['answer']}\" for qa in state[\"answers\"]]\n",
        "    )\n",
        "    report_prompt = \"\"\"Below are a bunch of questions and answers about topic:\n",
        "                       {topic}.\n",
        "                       Generate a detailed report from this about the topic.\n",
        "                       {q_and_a}\"\"\"\n",
        "    prompt = report_prompt.format(topic=state[\"topic\"], q_and_a=q_and_a)\n",
        "    response = llm.with_structured_output(Report).invoke(prompt)\n",
        "    return {\"report\": response.report}"
      ],
      "metadata": {
        "id": "hZ-LxJ5YRb82"
      },
      "id": "hZ-LxJ5YRb82",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Agent"
      ],
      "metadata": {
        "id": "yuRbxOMMYjMS"
      },
      "id": "yuRbxOMMYjMS"
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "# Compile the graph\n",
        "graph = StateGraph(OverallState)\n",
        "graph.add_node(\"generate_questions\", generate_questions)\n",
        "graph.add_node(\"generate_answer\", generate_answer)\n",
        "graph.add_node(\"compile_report\", compile_report)\n",
        "\n",
        "graph.add_edge(START, \"generate_questions\")\n",
        "# the following is not doing any conditional branching\n",
        "# except using the Send function to dynamically create N copies of generate_answer\n",
        "# where N = the number of questions (steps in the plan)\n",
        "graph.add_conditional_edges(\"generate_questions\",\n",
        "                            parallelize_answer_generation,\n",
        "                            [\"generate_answer\"])\n",
        "graph.add_edge(\"generate_answer\", \"compile_report\")\n",
        "graph.add_edge(\"compile_report\", END)\n",
        "\n",
        "# Compile the app\n",
        "agent = graph.compile()"
      ],
      "metadata": {
        "id": "1a1p_mpwRb_C"
      },
      "id": "1a1p_mpwRb_C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the graph\n",
        "from IPython.display import display, Image\n",
        "display(Image(agent.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "Te1lHnkqRcBH"
      },
      "id": "Te1lHnkqRcBH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run and Test the Agent"
      ],
      "metadata": {
        "id": "sUfJrGBAYv64"
      },
      "id": "sUfJrGBAYv64"
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "for state in agent.stream({\"topic\": \"Artificial Intelligence\"}):\n",
        "    print(state)\n",
        "    if 'compile_report' in state:\n",
        "        display(Markdown(state['compile_report']['report']))"
      ],
      "metadata": {
        "id": "UYB9crmZRcDD"
      },
      "id": "UYB9crmZRcDD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for state in agent.stream({\"topic\": \"Impact of AI on jobs\"}):\n",
        "    print(state)\n",
        "    if 'compile_report' in state:\n",
        "        display(Markdown(state['compile_report']['report']))"
      ],
      "metadata": {
        "id": "lbaRJSh9RcH_"
      },
      "id": "lbaRJSh9RcH_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ddra7VuHbiwn"
      },
      "id": "ddra7VuHbiwn",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}